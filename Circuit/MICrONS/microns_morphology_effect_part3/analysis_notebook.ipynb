{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5454729",
   "metadata": {},
   "source": [
    "# Microns morphology effect part 3\n",
    "\n",
    "This analysis continues the notebook titled \"microns morphology effect\" and \"microns morphology effect - part 2\". In those notebooks we demonstrated that in the MICrONS connectome connections are not formed statistically independently. Specifically: If a connection exists from neuron i to neuron j, then the probability that a connection exists from i to the nearest neighbor of j is increased. We also demonstrated that this can only partially be explained by a \"global\" effect based on long-tailed degree distributions. Instead, the effect has a complex spatial structure.\n",
    "\n",
    "**We suggest you first look at those two notebook if you have not already.**\n",
    "\n",
    "### More on the spatial structure of the morphology effect\n",
    "Here we investigate the spatial structure further.\n",
    "\n",
    "The previous analyses demonstrated dependence between a connection i -> j and i -> nearest neighbor of j. But what about other neurons around j? How far does the effect reach? Additionally, it is conceivable that innervation of one location makes innervation of another location **less** likely.\n",
    "\n",
    "We explore this in the following way. Instead of an overall mean, we consider the connection probability of *individual neurons* into given spatial bins. Then we consider the correlations of connection probabilities into pairs of spatial bins over individual neurons. That is, we ask: If a neuron has a high connection probability into a spatial bin at one location, is its connection probability into another bin increased or decreased? \n",
    "\n",
    "As in part 2, such an effect can be global, where an increase into all other bins is observed, and which can be explained by degree distributions; or local, with spatial structure and not captured by a configuration model.\n",
    "\n",
    "### Similar to part 2 - but different\n",
    "You will note that the structure of this notebook and its analyses is similar to part 2. But there are important differences. So please read the explanations and code carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1580ec",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We begin by importing relevant packages and setting up paths.\n",
    "\n",
    "### Customization options\n",
    "We have set up this notebook and the paths below for the analysis of the excitatory subgraph of the MICrONS connectome.\n",
    "\n",
    "But you can use it to analyze other connectomes as well, for example of the various circuit models we offer on our platform. You can download the .h5-formatted connectome from the platform and update the paths below accordingly. Contact us if you want assistance with this.\n",
    "\n",
    "### Caching the results\n",
    "This analysis is somewhat computationally expensive (~20 minutes for 50k neuron connectomes). Hence, we broke it up into two parts: First, we calculate a pandas.DataFrame that holds pre-digested data. Then we plot the data according to customizable specifications. \n",
    "\n",
    "The output of the first, expensive step is saved into a file, such that it does not have to be re-calculated: In future executions it is instead read from the file, unless you set the \"force_recalculation\" flag to True. \n",
    "\n",
    "Note that we have initialized the default cache file with all relevant results for your convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import conntility\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interactive\n",
    "\n",
    "# Location of the file holding the connectome information\n",
    "fn_connectivity = \"../../../../shared_data/MICrONS_SONATA/microns_mm3_connectome_EXC_from_sonata.h5\"\n",
    "\n",
    "# Location of the file for storing/caching results of the compuations. If it does not exists, it will be created.\n",
    "# The file below already has the relevant results pre-calculted.\n",
    "fn_digest_dataframe = \"../../../../shared_data/MICrONS_SONATA/microns_EXC_nn_morphology_effect_digest.h5\"\n",
    "\n",
    "\n",
    "# If you set this flag to True, expensive calculations will be repeated even if the result already exists in the cache file above.\n",
    "# Note: If you set this to True you will have to update \"fn_digest_dataframe\" to a different, local path. This is because the \n",
    "# default location of \"fn_digest_dataframe\" is on a read-only file system.\n",
    "force_recalculation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5adb59",
   "metadata": {},
   "source": [
    "## Selecting the connection matrix\n",
    "We read a connection matrix from an .h5 file. However, a file can contain more than one connection matrix. \n",
    "\n",
    "**Select the one to analyze from the dropdown.**\n",
    "\n",
    "If you have not updated \"fn_connectivity\" in the previous cell, you will find the following connection matriced:\n",
    "  - connectivity/data: The latest version of MICrONS\n",
    "  - connectivity/dist_dep: A distance-dependent control fitted against the MICrONS data\n",
    "  - connectivity/configuration_model: A configuration model control of the MICrONS data\n",
    "\n",
    "We suggest that you start with \"connectivity/data\". Then, as a comparison, also try \"connectivity/configuration_model\". Finally, you can try \"connectivity/dist_dep\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341efaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fn_connectivity, \"r\") as h5:\n",
    "    contents = []\n",
    "    prefixes = list(h5.keys())\n",
    "    for prefix in prefixes:\n",
    "        [contents.append(prefix + \"/\" + _k) for _k in h5[prefix].keys()]\n",
    "\n",
    "sel_matrix = widgets.Dropdown(options=contents, index=1, description=\"Select matrix\")\n",
    "display(sel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718c693",
   "metadata": {},
   "source": [
    "We load the selected connectivity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = str(sel_matrix.value)\n",
    "M = conntility.ConnectivityMatrix.from_h5(fn_connectivity, \n",
    "                                          prefix=sel_matrix.value.split(\"/\")[0],\n",
    "                                          group_name=sel_matrix.value.split(\"/\")[1])\n",
    "print(f\"Loaded {len(M)} nodes with {len(M.edges)} edges from {selected}\")\n",
    "\n",
    "col_y = \"y\"\n",
    "col_xz = [\"x\", \"z\"]\n",
    "# This flag indicates whether y indicates a \"depth\", i.e. is counted from the top of L1 (True), or a reverse depth, counted from the bottom of L6 (False).\n",
    "# This is relevatn purely for plotting purposes at the end.\n",
    "y_is_depth = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842f3ea",
   "metadata": {},
   "source": [
    "### Generate spatial bins\n",
    "We generate spatial bins for the offset of neurons pairs along the y-axis, and their distance in the x/z-plane.\n",
    "For the y-bins we ensure that they are centered around 0.\n",
    "\n",
    "We found 50 um to be a good bin size, but you can update it. In that case, you will have to set the \"force_recalculate\" flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to adjust the bin size of the final plots here.\n",
    "bin_sz = 50.0 # um\n",
    "\n",
    "\n",
    "def make_spatial_bins(M_h, cols, bin_sz):\n",
    "    _data = M_h.vertices[cols]\n",
    "    delta = _data.max() - _data.min()\n",
    "\n",
    "    sz = numpy.sqrt((delta.values ** 2).sum())\n",
    "    if len(delta) == 1: # case 1d: negative and positive bins\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([-bins[:0:-1], bins])\n",
    "    else: # case 2d: Only positive bins, but exclude 0 dist\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([[0, 1E-12], bins[1:]])\n",
    "    return bins\n",
    "\n",
    "dbins_xz = make_spatial_bins(M, col_xz, bin_sz)\n",
    "binid_xz = numpy.arange(0, len(dbins_xz) + 1)\n",
    "\n",
    "dbins_y = make_spatial_bins(M, [col_y], bin_sz)\n",
    "binid_y = numpy.arange(0, len(dbins_y) + 1)\n",
    "\n",
    "bin_centers_y = 0.5 * (dbins_y[:-1] + dbins_y[1:])\n",
    "bin_centers_xz = 0.5 * (dbins_xz[1:-1] + dbins_xz[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d312a78",
   "metadata": {},
   "source": [
    "## Main calculation\n",
    "\n",
    "This is where this notebook differs from \"microns morphology effect part 2\". Although we still break up the population into chunks and execute the analysis chunk by chunk.\n",
    "\n",
    "### Step 1\n",
    "Here, we calculate a DataFrame with the following structure:\n",
    "\n",
    "Each row represents a possible combination of:\n",
    "  - A numeric identifier of a presynaptic neuron (i)\n",
    "  - spatial bin of an offset from neuron i in the xz plane\n",
    "  - spatial bin of an offset along the y-axis from neuron i\n",
    "  - number of edges (synapses) from i to a neuron j in the indicated spatial bin\n",
    "\n",
    "The first four columns list the values of these four properties. **a fifth column then counts the number of pairs of neurons for that combination**.\n",
    "\n",
    "The difference to \"microns morphology effect part 2\" is that we keep the counts for individual presynaptic neurons separate, instead of summing them.\n",
    "\n",
    "Additionally, we calculate a second DataFrame where i instead indicates a postsynaptic neuron and the number of edges from j to i is counted.\n",
    "\n",
    "### Step 2\n",
    "We then use this DataFrame to calculate the following: For each pre- (post-) synaptic neuron the number of potential synaptic partners it has in each spatial bin. The connection probability of each neuron into each spatial bin. This connection probability is calculated separately for different minimum edge counts, i.e. if 1, 2, 3, ... edges is the minumum for considering a pair of neurons connected. This allows focusing on the structure of exceptionally strong connections.\n",
    "\n",
    "### Caching the results\n",
    "As before, if the result has already been calculated before, it is read from a file instead. Otherwise it is stored into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b37c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = M.matrix.tocsr()\n",
    "\n",
    "def for_chunk(chunk, direction=\"outgoing\"):\n",
    "    # Which offset bin the pairs fall into\n",
    "    if direction == \"outgoing\": # means: from chunk to all neurons\n",
    "        Dxz = cdist(M.vertices.iloc[chunk][col_xz], M.vertices[col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = M.vertices.iloc[chunk][[col_y]].values - M.vertices[[col_y]].values.transpose() # PRE X POST\n",
    "        # PRE - POST. If y_is_depth: Negative values -> downwards connection.\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1\n",
    "        \n",
    "        j, i = numpy.meshgrid(numpy.arange(len(M)), chunk) # chunk is i: presyn. => consider outgoing\n",
    "        node_id = i.flatten()\n",
    "        edge_count = mat[chunk].toarray().flatten()\n",
    "    elif direction == \"incoming\":\n",
    "        Dxz = cdist(M.vertices[col_xz], M.vertices.iloc[chunk][col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = M.vertices[[col_y]].values - M.vertices.iloc[chunk][[col_y]].values.transpose() # PRE X POST\n",
    "        # PRE - POST. If y_is_depth: Negative values -> downwards connection.\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Negative values -> upwards connection\n",
    "        \n",
    "        j, i = numpy.meshgrid(chunk, numpy.arange(len(M))) # chunk is j: postsyn. => consider incoming\n",
    "        node_id = j.flatten()\n",
    "        edge_count = mat[:, chunk].toarray().flatten()\n",
    "    \n",
    "    # Count instances of each\n",
    "    ret = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten(),\n",
    "        \"y\": Dy.flatten(),\n",
    "        \"edge_count\": edge_count,\n",
    "        \"node_id\": node_id,\n",
    "    }).value_counts()\n",
    "    ret.name = \"count\"\n",
    "    return ret\n",
    "\n",
    "def update_xz_y_bins(df):\n",
    "    cols = list(df.index.names)\n",
    "    df = df.reset_index()\n",
    "    df[\"xz\"] = bin_centers_xz[df[\"xz\"]]\n",
    "    df[\"y\"] = bin_centers_y[df[\"y\"]]\n",
    "    df = df.set_index(cols)\n",
    "    return df[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e14b1b",
   "metadata": {},
   "source": [
    "### Try the cache first\n",
    "\n",
    "Here, we test whether the result can already be found in the cache (\"digest\") file. If so and \"force_recalculation\" is False, we load it.\n",
    "\n",
    "Otherwise, we iterate over chunks of neurons performing the costly calculation. This may take 20-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_exists = False\n",
    "if not force_recalculation:\n",
    "    if os.path.isfile(fn_digest_dataframe):\n",
    "        with h5py.File(fn_digest_dataframe, \"r\") as h5:\n",
    "            if selected + \"/P_in\" in h5 and \\\n",
    "                selected + \"/N_in\" in h5 and \\\n",
    "                selected + \"/P_out\" in h5 and \\\n",
    "                selected + \"/P_in\" in h5:\n",
    "                    digest_exists = True\n",
    "\n",
    "if digest_exists:\n",
    "    P_in = pandas.read_hdf(fn_digest_dataframe, selected + \"/P_in\")\n",
    "    P_out = pandas.read_hdf(fn_digest_dataframe, selected + \"/P_out\")\n",
    "    N_in = pandas.read_hdf(fn_digest_dataframe, selected + \"/N_in\")\n",
    "    N_out = pandas.read_hdf(fn_digest_dataframe, selected + \"/N_out\")\n",
    "else:\n",
    "    df_for_outgoing = []\n",
    "    df_for_incoming = []\n",
    "\n",
    "    chunk_sz = 5000\n",
    "    chunking = numpy.arange(0, len(M) + chunk_sz, chunk_sz)\n",
    "\n",
    "    for a, b in tqdm.tqdm(list(zip(chunking[:-1], chunking[1:]))):\n",
    "        chunk = numpy.arange(a, numpy.minimum(b, len(M)))\n",
    "        df_for_outgoing.append(for_chunk(chunk, direction=\"outgoing\"))\n",
    "        df_for_incoming.append(for_chunk(chunk, direction=\"incoming\"))\n",
    "        \n",
    "    df_for_outgoing = pandas.concat(df_for_outgoing, axis=0).drop(-1, axis=0, level=\"xz\")\n",
    "    df_for_incoming = pandas.concat(df_for_incoming, axis=0).drop(-1, axis=0, level=\"xz\")\n",
    "\n",
    "    df_for_outgoing = update_xz_y_bins(df_for_outgoing)\n",
    "    df_for_incoming = update_xz_y_bins(df_for_incoming)\n",
    "\n",
    "    def conn_prob_for_thresholds(df_in):\n",
    "        tmp = df_in.pivot(index=\"edge_count\", columns=\"node_id\", values=\"count\").fillna(0)\n",
    "        return (tmp.loc[::-1].cumsum(axis=0) / tmp.sum(axis=0)).stack()\n",
    "\n",
    "    def simply_count(df_in):\n",
    "        return df_in.reset_index().groupby(\"node_id\")[\"count\"].sum()\n",
    "\n",
    "    P_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
    "    P_out = P_out.reorder_levels([2, 0, 1, 3])\n",
    "    N_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n",
    "\n",
    "    P_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
    "    P_in = P_in.reorder_levels([2, 0, 1, 3])\n",
    "    N_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n",
    "\n",
    "    P_in.to_hdf(fn_digest_dataframe, key=(selected + \"/P_in\"))\n",
    "    P_out.to_hdf(fn_digest_dataframe, key=(selected + \"/P_out\"))\n",
    "    N_in.to_hdf(fn_digest_dataframe, key=(selected + \"/N_in\"))\n",
    "    N_out.to_hdf(fn_digest_dataframe, key=(selected + \"/N_out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78542521",
   "metadata": {},
   "source": [
    "## Optional normalization\n",
    "\n",
    "The following cell is optional. Set the \"perform_normalization\" flag to True to execute it.\n",
    "\n",
    "The normalization subtracts from the connection probabilities of a neuron its mean connection probability over all spatial bins. This is related to the distinction between a \"global\" and a \"local\" effect of morphology on connectivity. The global effect is one that can be explained by long-tailed degree distributions of neurons: If connection probability into one spatial bin is high, this indicates that the neuron has a high out-degree. Hence, connection probabilities into all other bins will be globally increased, without strong spatial structure. Any spatial structure to the correlation results on top of that will be attributed to a local effect.\n",
    "\n",
    "Subtracting from each neuron its mean connection probability will remove the impact of the global effect and allows us to study the two types of effect separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c42390",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_normalization = False\n",
    "\n",
    "if perform_normalization:\n",
    "    mn_P_out = P_out.groupby([\"edge_count\", \"node_id\"]).mean()\n",
    "    mn_P_out = mn_P_out[pandas.MultiIndex.from_frame(P_out.index.to_frame()[[\"edge_count\", \"node_id\"]])]\n",
    "    mn_P_out.index = P_out.index\n",
    "    mn_P_in = P_in.groupby([\"edge_count\", \"node_id\"]).mean()\n",
    "    mn_P_in = mn_P_in[pandas.MultiIndex.from_frame(P_in.index.to_frame()[[\"edge_count\", \"node_id\"]])]\n",
    "    mn_P_in.index = P_in.index\n",
    "\n",
    "    P_out = (P_out / mn_P_out).dropna()\n",
    "    P_in = (P_in / mn_P_in).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd572877",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Here, we plot the results. \n",
    "From the pre-digested representation of the data we created above, a plot can be (relatively) rapidly created.\n",
    "\n",
    "We make the plot customizable. \n",
    "\n",
    "**Read these descriptions carefully to understand the plots**\n",
    "  - direction: Consider the effect on \"incoming\" or \"outgoing\" connectivity.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - required_corr: Minimum number of data points for a valid estimate or Pearson correlation. It can be argued that correlations between 2 samples are meaningless. Increase to avoid this.\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - bin_xz and bin_y: Select one spatial bin using the two dropdowns. What will be displayed is the Pearson correlations of connection probabilities into the selected bin and connection probabilities into *all possible* bins. The location of the selected bin will be indicated by a blue \"x\" mark. The correlation with the selected bin will always be 1.0 by definition.\n",
    "  - clim_max: Adjust how tight the limits of the color bar are set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d087cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def image_plot_function(direction, required_count, required_corr, thresh_pair, bin_xz, bin_y, clim_max):\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "    \n",
    "    idx = N.index[N >= required_count]\n",
    "    src_bin = (bin_xz, bin_y)\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    if src_bin not in ptgt:\n",
    "        ax.text(0.5, 0.5, \"Source bin empty!\", horizontalalignment=\"center\")\n",
    "        return\n",
    "    psrc = ptgt[src_bin]\n",
    "\n",
    "    def cc(data_in):\n",
    "        data_in = data_in.droplevel([0, 1])\n",
    "        idx = psrc.index.intersection(data_in.index)\n",
    "        if len(idx) > required_corr:\n",
    "            return numpy.corrcoef(data_in[idx], psrc[idx])[0, 1]\n",
    "        return numpy.nan\n",
    "    I = ptgt.groupby([\"xz\", \"y\"]).apply(cc, include_groups=False)\n",
    "    I = I.sort_index().unstack(\"xz\")\n",
    "    img = plt.imshow(I, clim=[-clim_max, clim_max],\n",
    "                     cmap=\"coolwarm\", extent=make_extent(I))\n",
    "    plt.colorbar(img, label=\"Pearson corr.\")\n",
    "    # dy = y(post) - y(pre). If y is depth then a values > 0 indicates a _downwards_ connection.\n",
    "    # Hence, for \"Incoming\" we want values > 0 towards the top of the plot. if y is not depth,\n",
    "    # then the other way around.\n",
    "    if y_is_depth == (direction == \"outgoing\"): # This is the opposite of part 2. I know. I apologize. But this is correct.\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "    ax.plot(bin_xz, bin_y, marker='x', color=\"blue\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50)\n",
    "sel_required_corr = widgets.IntSlider(min=2, max=100, value=10)\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1)\n",
    "sel_xz = widgets.Dropdown(options=sorted(bin_centers_xz), index=1)\n",
    "sel_y = widgets.Dropdown(options=sorted(bin_centers_y), index=int(len(bin_centers_y)/2))\n",
    "sel_clim = widgets.FloatSlider(min=0.01, max=1.0, step=0.01, value=0.5)\n",
    "\n",
    "interactive(image_plot_function, direction=sel_direction, required_count=sel_required_count,\n",
    "            required_corr=sel_required_corr,\n",
    "            thresh_pair=sel_thresh_pair, bin_xz=sel_xz, bin_y=sel_y, clim_max=sel_clim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c5a60",
   "metadata": {},
   "source": [
    "## Cluster structure of correlations\n",
    "\n",
    "Here, we perform one last analysis. We cluster the structure of correlations of spatial bins and depict the results. This reveals which spatial bins tend to be innervated together!\n",
    "\n",
    "**Options**\n",
    "  - direction: Consider the effect on \"incoming\" or \"outgoing\" connectivity.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - louvain_res: Value of the resolution parameter of Louvain clustering.\n",
    "  - min_cluster_sz: Cluster that comprise fewer than that number of spatial bins will be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f681b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sknetwork.clustering import Louvain\n",
    "from sknetwork.clustering import get_modularity\n",
    "from scipy import stats\n",
    "\n",
    "progress = widgets.IntProgress(min=0, max=10, value=0, description=\"Calculating...\")\n",
    "modularity_display = widgets.HTML(\n",
    "    value=\"...\",\n",
    "    description='Modularity',\n",
    ")\n",
    "\n",
    "def cluster_and_show(direction, required_count, thresh_pair, louvain_res, min_cluster_sz):\n",
    "    progress.value = 0\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    idx = N.index[N >= required_count]\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    progress.value = 2\n",
    "\n",
    "    cc = ptgt.sort_index().unstack(\"node_id\").transpose().corr().fillna(0)\n",
    "    cc[cc < 0] = 0\n",
    "    progress.value = 7\n",
    "\n",
    "    res = Louvain(louvain_res).fit_predict(cc.values)\n",
    "    clst = pandas.Series(res, index=cc.index)\n",
    "\n",
    "    vc = clst.value_counts()\n",
    "    invalid = vc.index[vc < min_cluster_sz]\n",
    "    clst[clst.isin(invalid)] = numpy.nan\n",
    "    I = clst.unstack(\"xz\")\n",
    "    progress.value = 9\n",
    "\n",
    "    ax.imshow(I, extent=make_extent(I), cmap=\"Set3\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "    if y_is_depth == (direction == \"outgoing\"): # This is the opposite of part 2. I know. I apologize. But this is correct.\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "\n",
    "    modularity_display.value = f\"<b>{get_modularity(cc.values, res, resolution=louvain_res)}</b>\"\n",
    "    progress.value = 10\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50, behavior=\"tap\")\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1, behavior=\"tap\")\n",
    "sel_louvain = widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=1.0, behavior=\"tap\")\n",
    "sel_min_sz = widgets.IntSlider(min=1, max=200, value=20, behavior=\"tap\")\n",
    "\n",
    "display(progress)\n",
    "display(modularity_display)\n",
    "interactive(cluster_and_show, direction=sel_direction, required_count=sel_required_count,\n",
    "            thresh_pair=sel_thresh_pair, louvain_res=sel_louvain, min_cluster_sz=sel_min_sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b34213",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "After running the analysis for all three connectomes and comparing them we can interpret the results.\n",
    "\n",
    "We note:\n",
    "  - There is a significant spatial extent to the correlations of connection probabilities for outgoing connections. In general, if a spatial bin is strongly innervated, the connection probabilities for bins ~150 um around it are elevated.\n",
    "  - For incoming connectivity the effect is present, but much weaker.\n",
    "  - In the data (without normalization), correlations are to the largest extent only positive. This contradicts a conceivable model where presence of axon at one location decreases its presence in other locations, because there is a limited amount of \"energy\" to spend on each axon. Instead it is compatible with a multiplicative model as proposed by Piazza and colleagues (https://doi.org/10.1101/2025.02.27.640551) \n",
    "  - The configuration model depicts strong correlations as well, but without specific spatial structure. This confirms the idea that there is a \"global\" effect that results from long-tailed degree distributions. However, correlations in the actual data has more specific spatial structure indicating that there is also a significant \"local\" effect. \n",
    "  - The \"local\" effect can be succesfully separated and visualized when the normalization is applied. The local effect also depicts negative correlations.\n",
    "  - There appears to be a weak(!) local effect visible after normalization even for the configuration model. This is unexpected. It can be explained by the edge effect of connectivity: Neurons near the boundaries of the volumes will have a lower degree as many of their connections are extrinsic, leading to a weak interaction between degree and locations of connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
