{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b9dbf3",
   "metadata": {},
   "source": [
    "# Microns morphology effect part 2\n",
    "\n",
    "This analysis continues the notebook titled \"microns morphology effect\". In that notebook we demonstrated that in the MICrONS connectome connections are not formed statistically independently. Specifically: If a connection exists from neuron i to neuron j, then the probability that a connection exists from i to the nearest neighbor of j is increased.\n",
    "\n",
    "A distance-dependent model did not -- and logically cannot(!) -- capture the effect.\n",
    "\n",
    "**We suggest you first look at that notebook if you have not already.**\n",
    "\n",
    "### Testing \"local\" vs. \"global\" effect\n",
    "It is possible that the nearest neighbor being connected merely indicates that the presynaptic neuron has a high out-degree, which then increases the connection probabilities with _all_ other neurons. We call this the \"global\" morphology effect.\n",
    "Alternatively, there can be a spatially limited effect, where the connection probability is increased in a specific area around the nearest neighbor. We call this a \"local\" morphology effect. \n",
    "\n",
    "The total effect can be a mixture of both, and in this notebook we conduct further analyses to further separate them. \n",
    "\n",
    "To that end we conduct an analysis of connection probabilities not just against distance, but against horizontal and vertical offsets from the pre-/post-synaptic neuron. \n",
    "The global effect will have no specific spatial structure, i.e., the increase in connection probability if the nearest neighbor is connected should be the same at each offset.\n",
    "Additionally, we analyze also the connectivity of a **configuration model** fitted against the data. That is a model that shuffles the locations of connections, but preserves the in- and out-degrees of all neurons. As the global effect is simply based on some neurons having a higher degree than others, the configuration model will capture it completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619ac87",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We begin by importing relevant packages and setting up paths.\n",
    "\n",
    "### Customization options\n",
    "We have set up this notebook and the paths below for the analysis of the excitatory subgraph of the MICrONS connectome.\n",
    "\n",
    "But you can use it to analyze other connectomes as well, for example of the various circuit models we offer on our platform. You can download the .h5-formatted connectome from the platform and update the paths below accordingly. Contact us if you want assistance with this.\n",
    "\n",
    "### Caching the results\n",
    "This analysis is somewhat computationally expensive (~20 minutes for 50k neuron connectomes). Hence, we broke it up into two parts: First, we calculate a pandas.DataFrame that holds pre-digested data. Then we plot the data according to customizable specifications. \n",
    "\n",
    "The output of the first, expensive step is saved into a file, such that it does not have to be re-calculated: In future executions it is instead read from the file, unless you set the \"force_recalculation\" flag to True. \n",
    "\n",
    "Note that we have initialized the default cache file with all relevant results for your convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import conntility\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interactive\n",
    "\n",
    "# Location of the file holding the connectome information\n",
    "fn_connectivity = \"../../../../shared_data/MICrONS_SONATA/microns_mm3_connectome_EXC_from_sonata.h5\"\n",
    "# Location of the file for storing/caching results of the compuations. If it does not exists, it will be created.\n",
    "# The file below already has the relevant results pre-calculted.\n",
    "fn_digest_dataframe = \"../../../../shared_data/MICrONS_SONATA/microns_EXC_nn_morphology_effect_digest.h5\"\n",
    "\n",
    "# If you set this flag to True, expensive calculations will be repeated even if the result already exists in the cache file above.\n",
    "# Note: If you set this to True you will have to update \"fn_digest_dataframe\" to a different, local path. This is because the \n",
    "# default location of \"fn_digest_dataframe\" is on a read-only file system.\n",
    "force_recalculation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c90aa",
   "metadata": {},
   "source": [
    "## Selecting the connection matrix\n",
    "We read a connection matrix from an .h5 file. However, a file can contain more than one connection matrix. \n",
    "\n",
    "**Select the one to analyze from the dropdown.**\n",
    "\n",
    "If you have not updated \"fn_connectivity\" in the previous cell, you will find the following connection matriced:\n",
    "  - connectivity/data: The latest version of MICrONS\n",
    "  - connectivity/dist_dep: A distance-dependent control fitted against the MICrONS data\n",
    "  - connectivity/configuration_model: A configuration model control of the MICrONS data\n",
    "\n",
    "We suggest that you start with \"connectivity/data\". Then, as a comparison, also try \"connectivity/configuration_model\". Finally, you can try \"connectivity/dist_dep\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fn_connectivity, \"r\") as h5:\n",
    "    contents = []\n",
    "    prefixes = list(h5.keys())\n",
    "    for prefix in prefixes:\n",
    "        [contents.append(prefix + \"/\" + _k) for _k in h5[prefix].keys()]\n",
    "\n",
    "sel_matrix = widgets.Dropdown(options=contents, index=1, description=\"Select matrix\")\n",
    "display(sel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8022d",
   "metadata": {},
   "source": [
    "We load the selected connectivity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = str(sel_matrix.value)\n",
    "M = conntility.ConnectivityMatrix.from_h5(fn_connectivity, \n",
    "                                          prefix=sel_matrix.value.split(\"/\")[0],\n",
    "                                          group_name=sel_matrix.value.split(\"/\")[1])\n",
    "print(f\"Loaded {len(M)} nodes with {len(M.edges)} edges from {selected}\")\n",
    "\n",
    "col_y = \"y\"\n",
    "col_xz = [\"x\", \"z\"]\n",
    "# This flag indicates whether y indicates a \"depth\", i.e. is counted from the top of L1 (True), or a reverse depth, counted from the bottom of L6 (False).\n",
    "# This is relevatn purely for plotting purposes at the end.\n",
    "y_is_depth = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398aa2",
   "metadata": {},
   "source": [
    "### Generate spatial bins\n",
    "We generate spatial bins for the offset of neurons pairs along the y-axis, and their distance in the x/z-plane.\n",
    "For the y-bins we ensure that they are centered around 0.\n",
    "\n",
    "We found 50 um to be a good bin size, but you can update it. In that case, you will have to set the \"force_recalculate\" flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to adjust the bin size of the final plots here.\n",
    "bin_sz = 50.0 # um\n",
    "\n",
    "\n",
    "def make_spatial_bins(M_h, cols, bin_sz):\n",
    "    _data = M_h.vertices[cols]\n",
    "    delta = _data.max() - _data.min()\n",
    "\n",
    "    sz = numpy.sqrt((delta.values ** 2).sum())\n",
    "    if len(delta) == 1: # case 1d: negative and positive bins\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([-bins[:0:-1], bins])\n",
    "    else: # case 2d: Only positive bins, but exclude 0 dist\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([[0, 1E-12], bins[1:]])\n",
    "    return bins\n",
    "\n",
    "dbins_xz = make_spatial_bins(M, col_xz, bin_sz)\n",
    "binid_xz = numpy.arange(0, len(dbins_xz) + 1)\n",
    "\n",
    "dbins_y = make_spatial_bins(M, [col_y], bin_sz)\n",
    "binid_y = numpy.arange(0, len(dbins_y) + 1)\n",
    "\n",
    "bin_centers_y = 0.5 * (dbins_y[:-1] + dbins_y[1:])\n",
    "bin_centers_xz = 0.5 * (dbins_xz[1:-1] + dbins_xz[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee189816",
   "metadata": {},
   "source": [
    "## Find nearest neighbors and calculate connectivity with nearest neighbors\n",
    "\n",
    "We use KDTrees to quickly find nearest neigbors of all neurons.\n",
    "\n",
    "Then we calculate three sparse matrices:\n",
    "  - The first simply holds the number of edges between neurons i and j\n",
    "  - The second holds the number of edges between i and the nearest neigbor of j\n",
    "  - The third holds the number of edges between the nearest neigbbor of i and j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "_coords = col_xz + [col_y]\n",
    "tree = KDTree(M.vertices[_coords].values)\n",
    "\n",
    "_, nn_id = tree.query(M.vertices[_coords], k=2)\n",
    "nn_id = nn_id[:, 1]  # nn_id[:, 0] is the original node, which has distance 0. nn_id[:, 1] is neighbor\n",
    "\n",
    "# Lookup from pre / post ids to number of edges\n",
    "pair_to_edge_count = M.edges.set_index(pandas.MultiIndex.from_frame(M._edge_indices))[\"count\"]\n",
    "\n",
    "# Edge counts i -> j\n",
    "edge_count_mat = M.matrix.tocsr()\n",
    "# Edge counts nn(i) -> j\n",
    "edge_count_nnpre_mat = edge_count_mat[nn_id]\n",
    "# Edge counts i -> nn(j)\n",
    "edge_count_nnpost_mat = edge_count_mat[:, nn_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59809d2",
   "metadata": {},
   "source": [
    "## Main calculation\n",
    "\n",
    "Here we calculate a DataFrame with the following structure:\n",
    "\n",
    "Each row represents a possible combination of:\n",
    "  - spatial bin of the offset in the xz plane of a pair of neurons (i - j)\n",
    "  - spatial bin of their offset along the y-axis\n",
    "  - number of edges (synapses) from i to j\n",
    "  - number of edges (synapses) from i to the nearest neighbor of j\n",
    "\n",
    "The first four columns list the values of these four properties. **a fifth column then counts the number of pairs of neurons for that combination**.\n",
    "\n",
    "Additionally, we calculate a second DataFrame that is very similar, except the fourth column represents instead the number of edges (synapses) from the nearest neighbor of i to j.\n",
    "\n",
    "\n",
    "For this calculation we have to consider the distances between all pairs of neurons. For large connectomes this can result in a very large matrix. To avoid running out of memory on weaker machines, we conduct the analysis in chunks of 2500 neurons. In the next cell, we define the function we run for each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the analysis for a given set of _rows_ of the connectivity matrix\n",
    "def for_pre_chunk(chunk_pre):\n",
    "    # Which offset bin the pairs fall into\n",
    "    Dxz = cdist(M.vertices.iloc[chunk_pre][col_xz], M.vertices[col_xz]) # PRE X POST\n",
    "    Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1. That is the one to exclude.\n",
    "\n",
    "    Dy = -M.vertices.iloc[chunk_pre][[col_y]].values + M.vertices[[col_y]].values.transpose() # PRE X POST\n",
    "    Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Values are post - pre, i.e. the delta y along the direction of connection\n",
    "    # Numer of edges i -> j\n",
    "    edge_count = edge_count_mat[chunk_pre].toarray().flatten()\n",
    "\n",
    "    # Number of edges nn(i) -> j\n",
    "    edge_count_nnpre = edge_count_nnpre_mat[chunk_pre].toarray().flatten()\n",
    "    # is nn(i) == j?\n",
    "    collision_pre = (nn_id[chunk_pre].reshape((-1, 1)) - numpy.arange(edge_count_mat.shape[1]).reshape((1, -1))) != 0\n",
    "    collision_pre = collision_pre.flatten()\n",
    "\n",
    "    # Number of edges i -> nn(j)\n",
    "    edge_count_nnpost = edge_count_nnpost_mat[chunk_pre].toarray().flatten()\n",
    "    # is i == nn(j)?\n",
    "    collision_post = (chunk_pre.reshape((-1, 1)) - nn_id.reshape((1, -1))) != 0\n",
    "    collision_post = collision_post.flatten()\n",
    "\n",
    "    # Count instances of each\n",
    "    ret_incoming = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_pre],\n",
    "        \"y\": Dy.flatten()[collision_pre],\n",
    "        \"edges_pair\": edge_count[collision_pre],\n",
    "        \"edges_nn\": edge_count_nnpre[collision_pre],\n",
    "    }).value_counts()\n",
    "    \n",
    "    ret_outgoing = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_post],\n",
    "        \"y\": Dy.flatten()[collision_post],\n",
    "        \"edges_pair\": edge_count[collision_post],\n",
    "        \"edges_nn\": edge_count_nnpost[collision_post],\n",
    "    }).value_counts()\n",
    "    return ret_incoming, ret_outgoing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66345460",
   "metadata": {},
   "source": [
    "### Try the cache first\n",
    "\n",
    "Here, we test whether the result can already be found in the cache (\"digest\") file. If so and \"force_recalculation\" is False, we load it.\n",
    "\n",
    "Otherwise, we iterate over chunks of neurons performing the costly calculation. This may take 20-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_exists = False\n",
    "if not force_recalculation:\n",
    "    if os.path.isfile(fn_digest_dataframe):\n",
    "        with h5py.File(fn_digest_dataframe, \"r\") as h5:\n",
    "            if selected + \"/incoming\" in h5 and selected + \"/outgoing\" in h5:\n",
    "                digest_exists = True\n",
    "\n",
    "if digest_exists:\n",
    "    full_df_incoming = pandas.read_hdf(fn_digest_dataframe, selected + \"/incoming\")\n",
    "    full_df_outgoing = pandas.read_hdf(fn_digest_dataframe, selected + \"/outgoing\")\n",
    "else:\n",
    "    chunk_sz = 2500\n",
    "    chunking = numpy.arange(0, len(M) + chunk_sz, chunk_sz)\n",
    "\n",
    "    chunk = numpy.arange(chunking[0], numpy.minimum(chunking[1], len(M)))\n",
    "    full_df_incoming, full_df_outgoing = for_pre_chunk(chunk)\n",
    "\n",
    "    for a, b in tqdm.tqdm(list(zip(chunking[1:-1], chunking[2:]))):\n",
    "        chunk = numpy.arange(a, numpy.minimum(b, len(M)))\n",
    "        new_df_in, new_df_out = for_pre_chunk(chunk)\n",
    "        full_df_incoming = full_df_incoming.add(new_df_in, fill_value=0)\n",
    "        full_df_outgoing = full_df_outgoing.add(new_df_out, fill_value=0)\n",
    "    \n",
    "    full_df_incoming = full_df_incoming.drop(-1, axis=0).reset_index()\n",
    "    full_df_outgoing = full_df_outgoing.drop(-1, axis=0).reset_index()\n",
    "\n",
    "\n",
    "    assert (full_df_incoming[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "    assert (full_df_outgoing[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "\n",
    "    assert (full_df_incoming[\"xz\"] < len(binid_xz)).all()\n",
    "    assert (full_df_incoming[\"y\"] < len(binid_y)).all()\n",
    "    assert (full_df_outgoing[\"xz\"] < len(binid_xz)).all()\n",
    "    assert (full_df_outgoing[\"y\"] < len(binid_y)).all()\n",
    "\n",
    "    full_df_incoming[\"xz\"] = bin_centers_xz[full_df_incoming[\"xz\"]]\n",
    "    full_df_incoming[\"y\"] = bin_centers_y[full_df_incoming[\"y\"]]\n",
    "\n",
    "    full_df_outgoing[\"xz\"] = bin_centers_xz[full_df_outgoing[\"xz\"]]\n",
    "    full_df_outgoing[\"y\"] = bin_centers_y[full_df_outgoing[\"y\"]]\n",
    "\n",
    "    full_df_incoming.to_hdf(fn_digest_dataframe, key=(selected + \"/incoming\"))\n",
    "    full_df_outgoing.to_hdf(fn_digest_dataframe, key=(selected + \"/outgoing\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4f14a",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Here, we plot the results. As in the notebook \"microns morphology effect\", we plot the overall (prior) connection probabilies in spatial bins, and the (posterior) connection probability, conditional on the nearest neighbor of a neuron being connected.\n",
    "\n",
    "From the pre-digested representation of the data we created above, a plot can be rapidly created.\n",
    "\n",
    "We make the plot customizable:\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - thresh_nn: Similar to the above. This is the threshold of synapse count for considering the nearest neighbor connected.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - clim_max: Adjust how tight the limits of the color bar are set.\n",
    "  - show_relative: If unchecked, then the raw difference of posterior and prior connection probability is plotted. Otherwise, their relative difference is plotted (Michaelson contrast, i.e., bounded between -1 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_prior_post_fun(df_in, thresh_pair=1, thresh_nn=1, required_count=1):\n",
    "    v_pair = df_in[\"edges_pair\"] >= thresh_pair\n",
    "    v_nn = df_in[\"edges_nn\"] >= thresh_nn\n",
    "\n",
    "    if df_in.loc[v_nn, \"count\"].sum() < required_count:\n",
    "        return pandas.Series({\n",
    "            \"prior\": numpy.nan,\n",
    "            \"posterior\": numpy.nan\n",
    "        })\n",
    "\n",
    "    prior = df_in.loc[v_pair, \"count\"].sum() / df_in[\"count\"].sum()\n",
    "    \n",
    "    df_in = df_in.loc[v_nn]\n",
    "    v_pair = v_pair[v_nn]\n",
    "    posterior = 0.0\n",
    "    if numpy.any(v_pair):\n",
    "        posterior = df_in.loc[v_pair][\"count\"].sum() / df_in[\"count\"].sum()\n",
    "    return pandas.Series({\n",
    "        \"prior\": prior,\n",
    "        \"posterior\": posterior\n",
    "    })\n",
    "\n",
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def show_results(res_df_incoming, res_df_outgoing, clim=[0, 0.1],\n",
    "                 show_relative=False):\n",
    "    fig = plt.figure(figsize=(4, 6))\n",
    "\n",
    "    i = 1\n",
    "    for prob_type in [\"prior\", \"posterior\"]:\n",
    "        for df, df_str in zip([res_df_incoming, res_df_outgoing],\n",
    "                              [\"Incoming\", \"Outgoing\"]):\n",
    "            ax = fig.add_subplot(3, 2, i)\n",
    "            img = df[prob_type].sort_index().unstack(\"xz\")\n",
    "            pltimg = ax.imshow(img, extent=make_extent(img), clim=clim)\n",
    "            ax.set_frame_on(False)\n",
    "            ax.set_title(f\"{df_str} connections\", fontsize=10)\n",
    "            # dy = y(post) - y(pre). If y is depth then a values > 0 indicates a _downwards_ connection.\n",
    "            # Hence, for \"Incoming\" we want values > 0 towards the top of the plot. if y is not depth,\n",
    "            # then the other way around.\n",
    "            if y_is_depth == (df_str == \"Incoming\"):\n",
    "                ax.set_ylim(sorted(ax.get_ylim()))\n",
    "            ax.set_xticks([])\n",
    "            if numpy.mod(i, 2) == 0:\n",
    "                plt.colorbar(pltimg, label=f\"{prob_type} prob.\")\n",
    "                ax.set_yticks([])\n",
    "            i += 1\n",
    "\n",
    "    clim_diff = [-clim[1], clim[1]]\n",
    "    for df, df_str in zip([res_df_incoming, res_df_outgoing],\n",
    "                              [\"Incoming\", \"Outgoing\"]):\n",
    "        ax = fig.add_subplot(3, 2, i)\n",
    "        img = df[\"posterior\"].subtract(df[\"prior\"], fill_value=0)\n",
    "        if show_relative:\n",
    "            img = img.divide(df[\"prior\"].add(df[\"posterior\"], fill_value=0), fill_value=0)\n",
    "            clim_diff = [-1.0, 1.0]\n",
    "        img = img.sort_index().unstack(\"xz\")\n",
    "            \n",
    "        pltimg = ax.imshow(img, extent=make_extent(img), clim=clim_diff, cmap=\"coolwarm\")\n",
    "        ax.set_frame_on(False)\n",
    "        ax.set_title(f\"Difference\", fontsize=10)\n",
    "        if y_is_depth == (df_str == \"Incoming\"):\n",
    "            ax.set_ylim(sorted(ax.get_ylim()))\n",
    "        ax.set_xticks(ax.get_xticks()); ax.set_xticklabels(ax.get_xticks(), rotation=\"vertical\")\n",
    "        if numpy.mod(i, 2) == 0:\n",
    "            plt.colorbar(pltimg)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def interact_fun(thresh_pair, thresh_nn, required_count, clim_max, show_relative):\n",
    "    res_in = full_df_incoming.groupby([\"xz\", \"y\"]).apply(p_prior_post_fun,  include_groups=False,\n",
    "                                                    thresh_pair=thresh_pair,\n",
    "                                                    thresh_nn=thresh_nn,\n",
    "                                                    required_count=required_count)\n",
    "    res_out = full_df_outgoing.groupby([\"xz\", \"y\"]).apply(p_prior_post_fun,  include_groups=False,\n",
    "                                                    thresh_pair=thresh_pair,\n",
    "                                                    thresh_nn=thresh_nn,\n",
    "                                                    required_count=required_count)\n",
    "    show_results(res_in, res_out, clim=[0, clim_max], show_relative=show_relative)\n",
    "\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=10, value=1)\n",
    "sel_thresh_nn = widgets.IntSlider(min=1, max=10, value=1)\n",
    "sel_required_count = widgets.IntSlider(min=1, max=100, value=1)\n",
    "sel_clim_max = widgets.FloatSlider(min=0.01, max=1.0, value=0.1, step=0.01)\n",
    "sel_relative = widgets.Checkbox(value=False)\n",
    "\n",
    "interactive(interact_fun, thresh_pair=sel_thresh_pair, thresh_nn=sel_thresh_nn,\n",
    "            clim_max=sel_clim_max, required_count=sel_required_count, show_relative=sel_relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ce5d2",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "After running the analysis for all three connectomes and comparing them we can interpret the results.\n",
    "\n",
    "We note:\n",
    "  - There is no effect for the distance-dependent control. This is expected\n",
    "  - The effect is much stronger for outgoing than incoming connectivity. This is in line with the result of the \"microns morphology effect\" notebook.\n",
    "  - For the MICrONS data we find a complex spatial structure to the magnitude of the effect. This indicates that what we initially called the \"global\" effect does not fully explain the results.\n",
    "  - For the configuration model control we find an effect, but it lacks the spatial structure observed for the data. This is expected.\n",
    "  - For the configuration model control the effect is weaker than for the MICrONS data. This confirms that the \"global\" effect does not fully explain the results.\n",
    "  - For the MICrONS data, the strength of the effect increases when increase the minimum number of edges required for the nearest neighbor connection. This means that if a pair is not only connected, but is connected with multiple synapses, then the probability that the nearest neighbor is as connected as well is even more increased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entitysdk_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
