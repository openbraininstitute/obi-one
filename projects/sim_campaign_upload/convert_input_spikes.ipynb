{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c93692-5cd5-44f0-af76-ed9db254e974",
   "metadata": {},
   "source": [
    "# Convert input spikes of a simulation campaign from .dat to .h5 format\n",
    "\n",
    "- Incl. patching the `simulation_config.json` files to point to the new .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa05894-0012-4934-b638-93f0866ca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from shutil import copyfile, move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086cf441-0827-409a-b39a-a41724890b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root = Path(\"/Users/pokorny/Data\")\n",
    "data_root = Path(\"/Users/james/Documents/obi/Data\")\n",
    "\n",
    "# CAMPAIGN_PATH = data_root / Path(\"Simulations/BBP-curated/SM-whisker-scan\")\n",
    "# SPIKE_PATH = data_root / Path(\"Simulations/BBP-raw/SM-whisker-scan/518fbeaf-f0ba-4f99-a0ff-22cb7e89eed7\")\n",
    "CAMPAIGN_PATH = data_root / Path(\"Simulations/BBP-curated/assemblies\")\n",
    "SPIKE_PATH = data_root / Path(\"Simulations/BBP-raw/assemblies/d21efd45-7740-4268-b06f-e1de35f2b6cf\")\n",
    "\n",
    "SIM_CONFIG_NAME = \"simulation_config.json\"\n",
    "STIM_NAME = \"Stimulus spikeReplay\"\n",
    "STIM_POPULATION = \"VPM\"\n",
    "\n",
    "# CONVERTER_TOOL = \"/Users/pokorny/JupyterLab/git/libsonatareport/build/tools/converter/spikes_converter\"\n",
    "CONVERTER_TOOL = \"/Users/james/Documents/obi/code/libsonatareport/build/tools/converter/spikes_converter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5909d964-90b4-4256-b834-81e57c096e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 simulation(s) in campaign\n"
     ]
    }
   ],
   "source": [
    "# Check sub-folders\n",
    "sim_folders = list(CAMPAIGN_PATH.glob(\"*[0-9]\"))\n",
    "sim_folder_names = [f.name for f in sim_folders]\n",
    "sim_configs = [_sim / SIM_CONFIG_NAME for _sim in sim_folders]\n",
    "num_sims = len(sim_folders)\n",
    "print(f\"Found {num_sims} simulation(s) in campaign\")\n",
    "assert all(str(idx) in sim_folder_names for idx in range(num_sims)), \"ERROR: Subfolder names do not match simulation indices!\"\n",
    "assert all(_cfg.is_file() for _cfg in sim_configs), \"ERROR: Simulation config(s) not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3c2520-a1ec-477a-8b07-9a259b3772d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check spike files\n",
    "input_spike_files = []\n",
    "output_spike_files = []\n",
    "for idx, cfg in enumerate(sim_configs):\n",
    "    with open(cfg, \"r\") as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    inp_file = cfg_dict[\"inputs\"][STIM_NAME][\"spike_file\"]\n",
    "    assert Path(inp_file).suffix.lower() == \".dat\", \"ERROR: .dat input expected!\"\n",
    "    inp_path = SPIKE_PATH / str(idx) / inp_file\n",
    "    assert inp_path.is_file(), f\"ERROR: Input spike file '{inp_path}' not found!\"\n",
    "    out_file = Path(inp_file).stem + \".h5\"\n",
    "    out_path = CAMPAIGN_PATH / str(idx) / out_file\n",
    "    assert not out_path.is_file(), f\"ERROR: Output spike file '{out_path}' already exists!\"\n",
    "    input_spike_files.append(inp_path)\n",
    "    output_spike_files.append(out_path)\n",
    "    print(f\"Simulation {idx}: Spike file '{inp_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296b4377-9c7e-419f-b570-22a3d1f5769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversion\n",
    "for idx, (inp, out) in enumerate(zip(input_spike_files, output_spike_files)):\n",
    "    print(f\"Simulation {idx}: Converting '{inp}' -> '{out}'...\")\n",
    "\n",
    "    # Run conversion\n",
    "    convert_out = subprocess.check_output(f\"{CONVERTER_TOOL} {inp} {STIM_POPULATION}\", shell=True, text=True)\n",
    "    print(convert_out)\n",
    "\n",
    "    # Move converted file from currend directory to destination folder\n",
    "    converted_file = inp.name + \".h5\"\n",
    "    assert Path(converted_file).is_file(), \"ERROR: Converted file not found!\"\n",
    "    move(src=converted_file, dst=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3b3a35-1a43-4f5b-855c-ef51c3c4a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch simulation configs\n",
    "for idx, cfg in enumerate(sim_configs):\n",
    "    # Create .BAK\n",
    "    bak_file = cfg.parent / (cfg.stem + \".BAK\")\n",
    "    if not bak_file.is_file():\n",
    "        # Create only if not existing yet (to avoid overwriting)\n",
    "        copyfile(src=cfg, dst=bak_file)\n",
    "\n",
    "    # Patch config\n",
    "    with open(cfg, \"r\") as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    \n",
    "    for STIM_NAME in STIM_NAMES:\n",
    "        spk_file = cfg_dict[\"inputs\"][STIM_NAME][\"spike_file\"]\n",
    "        assert spk_file == input_spike_files[idx].name, \"ERROR: Spike file name mismatch!\"\n",
    "        spk_file_new = output_spike_files[idx].name\n",
    "        cfg_dict[\"inputs\"][STIM_NAME][\"spike_file\"] = spk_file_new\n",
    "    \n",
    "    with open(cfg, \"w\") as f:\n",
    "        json.dump(cfg_dict, f, indent=2)\n",
    "    print(f\"Simulation {idx}: Config file '{cfg}' written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137afce",
   "metadata": {},
   "source": [
    "# Optionally split nbS1 inputs into two files (VPM and POm)\n",
    "For old BlueConfig based sims in which inputs were in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c0d35df-a371-4cd4-919e-f4341eab0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEY\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ERROR: Output spike file '/Users/james/Documents/obi/Data/Simulations/BBP-curated/assemblies/0/input.h5' not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m spike_file = CAMPAIGN_PATH / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33minput.h5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m output_spike_root = CAMPAIGN_PATH / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m spike_file.is_file(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: Output spike file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspike_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mh5py\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: ERROR: Output spike file '/Users/james/Documents/obi/Data/Simulations/BBP-curated/assemblies/0/input.h5' not found!"
     ]
    }
   ],
   "source": [
    "thalamic_input_files = output_spike_files\n",
    "\n",
    "print(\"HEY\")\n",
    "\n",
    "for i in range(5):\n",
    "    spike_file = CAMPAIGN_PATH / f\"{i}\" / \"input.h5\"\n",
    "    output_spike_root = CAMPAIGN_PATH / f\"{i}\"\n",
    "\n",
    "    assert spike_file.is_file(), f\"ERROR: Output spike file '{spike_file}' not found!\"\n",
    "\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "\n",
    "    VPM_LO = 5_000_000\n",
    "    VPM_HI = 6_000_000  # exclusive\n",
    "    \n",
    "    vpm_out = output_spike_root / (\"input_VPM.h5\")\n",
    "    pom_out = output_spike_root / (\"input_POm.h5\")\n",
    "\n",
    "    # vpm_out = spike_file.with_name(spike_file.stem + \"_VPM.h5\")\n",
    "    # pom_out = spike_file.with_name(spike_file.stem + \"_POm.h5\")\n",
    "\n",
    "    with h5py.File(spike_file, \"r\") as fin:\n",
    "        node_ids = fin[\"spikes/VPM/node_ids\"][:]       # (N,)\n",
    "        timestamps = fin[\"spikes/VPM/timestamps\"][:]   # (N,)\n",
    "\n",
    "        # Masks\n",
    "        vpm_mask = (node_ids >= VPM_LO) & (node_ids < VPM_HI)\n",
    "        pom_mask = ~vpm_mask\n",
    "\n",
    "    # -------- VPM FILE --------\n",
    "    with h5py.File(vpm_out, \"w\") as fout:\n",
    "        g_spikes = fout.create_group(\"spikes\")\n",
    "        g_vpm = g_spikes.create_group(\"VPM\")\n",
    "\n",
    "        g_vpm.create_dataset(\"node_ids\", data=node_ids[vpm_mask])\n",
    "        g_vpm.create_dataset(\"timestamps\", data=timestamps[vpm_mask])\n",
    "\n",
    "    # -------- POm FILE --------\n",
    "    with h5py.File(pom_out, \"w\") as fout:\n",
    "        g_spikes = fout.create_group(\"spikes\")\n",
    "        g_pom = g_spikes.create_group(\"POm\")\n",
    "\n",
    "        g_pom.create_dataset(\"node_ids\", data=node_ids[pom_mask])\n",
    "        g_pom.create_dataset(\"timestamps\", data=timestamps[pom_mask])\n",
    "\n",
    "    print(f\"Input file:   {spike_file}\")\n",
    "    print(f\"VPM file:     {vpm_out} ({vpm_mask.sum()} spikes)\")\n",
    "    print(f\"POm file:     {pom_out} ({pom_mask.sum()} spikes)\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d390",
   "metadata": {},
   "source": [
    "# Optionally create new output spike file with \"S1nonbarrel_neurons\" name\n",
    "(For old BlueConfig sims) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaac09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "map_spikes_to_new_node_ids = True\n",
    "\n",
    "for idx, cfg in enumerate(sim_configs):\n",
    "    inp_path = SPIKE_PATH / str(idx) / \"out.h5\"\n",
    "\n",
    "    with h5py.File(inp_path, \"r\") as fin:\n",
    "        node_ids = fin[\"spikes/All/node_ids\"][:]       # (N,)\n",
    "        timestamps = fin[\"spikes/All/timestamps\"][:]   # (N,)\n",
    "\n",
    "\n",
    "    new_node_ids = node_ids\n",
    "    if map_spikes_to_new_node_ids:\n",
    "        \n",
    "\n",
    "    reporting_dir = CAMPAIGN_PATH / str(idx) / \"reporting\"\n",
    "    reporting_dir.mkdir(exist_ok=True)\n",
    "    with h5py.File(reporting_dir / \"spikes.h5\", \"w\") as f:\n",
    "\n",
    "        g_spikes = f.create_group(\"spikes\")\n",
    "        g_pom = g_spikes.create_group(\"S1nonbarrel_neurons\")\n",
    "\n",
    "        g_pom.create_dataset(\"node_ids\", data=new_node_ids)\n",
    "        g_pom.create_dataset(\"timestamps\", data=timestamps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obi-one (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
